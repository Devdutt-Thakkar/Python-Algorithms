{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7ff787",
   "metadata": {},
   "source": [
    "List comprehensions allow us to quickly generate lists using the logic of loops and iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae362750",
   "metadata": {},
   "source": [
    "They are optimized to run very quickly in python and they are easy to write concisely, but they can be hard to read and their syntax takes some getting used to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc6864",
   "metadata": {},
   "source": [
    "A corpus is a body of text that will 'teach' our algorithm the language and style we want it to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0223ed",
   "metadata": {},
   "source": [
    "In order to use algorithms with language, we must either make lan-\n",
    "guage simpler, so that the short mathematical algorithms we have explored\n",
    "so far can reliably work with it, or make our algorithms smarter, so that they\n",
    "can deal with the messy complexity of human language as it has developed\n",
    "naturally. Weâ€™ll do the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc791b73",
   "metadata": {},
   "source": [
    "# Space insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1a4b3",
   "metadata": {},
   "source": [
    "Suppose we are only given the digitized text that has a few errors in the text and we are not even given the paper record from which it has been extracted.\n",
    "\n",
    "how do we correct the mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e63633",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The oneperfectly divine thing, the oneglimpse of God's paradisegiven on earth, is to fight a losingbattle - and notlose it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d67cf",
   "metadata": {},
   "source": [
    "In the above example, even though the spellings are all correct, there needs to be spaces between certain words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a269bc",
   "metadata": {},
   "source": [
    "# Defining a word list and finding words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08cb0e",
   "metadata": {},
   "source": [
    "The first thing we will do for our algorithm is teach it some english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be73a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['The','one', 'perfectly', 'divine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb4380",
   "metadata": {},
   "source": [
    "### In this chapter we will create and manipulate list comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2d44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_copy = [word for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fa9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple example which can be made more complex as follows\n",
    "\n",
    "has_n = [word for word in word_list if 'n' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e43ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'divine']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18dda1",
   "metadata": {},
   "source": [
    "# We will be using re to access text manipulation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41673a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "locs = list(set([(m.start(), m.end()) for word in word_list for m in re.finditer(word, text)])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ee7da",
   "metadata": {},
   "source": [
    "the locs variable will contain the locations in the text of every word in our word list. we will use a list comprehension to get the list of locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636aad8",
   "metadata": {},
   "source": [
    "we use for word in word list to iterate over every word in word list.\n",
    "\n",
    "for each word we call re.finditer() which finds the selected words in our text and returns a list of every location where that word occurs. We iterate over these locations, and each individual location is stored in m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2dc67",
   "metadata": {},
   "source": [
    "we will get the location in the text of the beginning and end of the word, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c0cb",
   "metadata": {},
   "source": [
    "the whole list comprehension is enveloped by list(set()). This is a convenient way to get a list that contains only unique values with no duplicates. Our list comprehension alone might have multiple identical elements, but converting it to a set automatically removes duplicates, and then converting it back to a list puts in the format we want: a list of unique word locations. We can just run print(locs) to see the result of the whole operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86aab138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 23), (7, 16), (0, 3), (35, 38), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63071513",
   "metadata": {},
   "source": [
    "In python, the ordered pairs like these are called tuples, and these tuples show the location of each word from word_list in our text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abd215",
   "metadata": {},
   "source": [
    "as we know that some of these words end in the same index where other word begins, in order to find places where we neet to insert spaces, we will look for cases like this: where the end of one valid word is at the same place as the beginning of another valid word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b14b7",
   "metadata": {},
   "source": [
    "# Dealing with compound words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b585b",
   "metadata": {},
   "source": [
    "what if there is a word like butterfly where both butter and fly is valid but it dont mean butterfly invalid, hence we dont just need to change valid words that appear together but also need to check whether the words together make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a9457",
   "metadata": {},
   "source": [
    "In order to check this, we need to find all the spaces in our text. We can look at all the substrings between two consecutive spaces and call those potential words. If a potential word is not in our list, then we will conclude its invalid and check whether its made oj joint valid words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033eebd8",
   "metadata": {},
   "source": [
    "now we will again use re.finditer to find all spaces in text and store it in a variable called space starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6be1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts = [m.start() for m in re.finditer(' ', text)]\n",
    "spacestarts.append(-1)\n",
    "spacestarts.append(len(text))\n",
    "spacestarts.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376233f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " 3,\n",
       " 16,\n",
       " 23,\n",
       " 30,\n",
       " 34,\n",
       " 45,\n",
       " 48,\n",
       " 54,\n",
       " 68,\n",
       " 71,\n",
       " 78,\n",
       " 81,\n",
       " 84,\n",
       " 90,\n",
       " 92,\n",
       " 105,\n",
       " 107,\n",
       " 111,\n",
       " 119,\n",
       " 123]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d361203",
   "metadata": {},
   "source": [
    "the -1 initializes the frst word and the len of text shows the end of last word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bc0d9",
   "metadata": {},
   "source": [
    "it will be useful to have another list that records the locations of the first character of each potential word. \n",
    "\n",
    "we will call that list spacestart_affine, since in technical terms, this new list is an affine transformation of the spacestarts list.\n",
    "\n",
    "Affine is often used to refer to linear transformations, such as adding 1 to each location, which we will do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd1a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts_affine = [ss+1 for ss in spacestarts]\n",
    "spacestarts_affine.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4bf98c",
   "metadata": {},
   "source": [
    "Next we will get all the substrings that are between two spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99ab3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces = [(spacestarts[k] + 1, spacestarts[k+1]) for k in range(0, len(spacestarts) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6a75f",
   "metadata": {},
   "source": [
    "The variable is a list of tuple of the form (location of beginning of substrings, location of end of substring) like (17,23). The way we get these tuples through a list comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67ab42",
   "metadata": {},
   "source": [
    "This list comprehension iterates over k. In this case, k takes on the values of integers between 0 and one less than the length of the spacestarts list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00592267",
   "metadata": {},
   "source": [
    "for each k we will generate 1 tuple.\n",
    "\n",
    "the first element of the tuple is spacestarts[k] + 1, which is one position after the location of each space. The seconf element of the tuple is spacestarts[k+1] which is the location of the next space in the text. this way our final output contains tuples that indicate the beginning and end of each substring between spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb51cce",
   "metadata": {},
   "source": [
    "Now consider all of the potential words that are between spaces, and find the ones that are no valid ( not in our list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a327ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d812074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 16),\n",
       " (24, 30),\n",
       " (31, 34),\n",
       " (35, 45),\n",
       " (46, 48),\n",
       " (49, 54),\n",
       " (55, 68),\n",
       " (69, 71),\n",
       " (72, 78),\n",
       " (79, 81),\n",
       " (82, 84),\n",
       " (85, 90),\n",
       " (91, 92),\n",
       " (93, 105),\n",
       " (106, 107),\n",
       " (108, 111),\n",
       " (112, 119),\n",
       " (120, 123)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "between_spaces_notvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68d2a1",
   "metadata": {},
   "source": [
    "looking here we can see that its a list of the locations of all invalid potential words in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db25076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oneperfectly'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[4:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd62904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thing,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[24:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd23e8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[31:34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d53291",
   "metadata": {},
   "source": [
    "One way to make all words recognized is by manually adding or we can simply import a word list that already contained a substantial body of valid english words. Such a collection of words is referred to as corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d08f7f",
   "metadata": {},
   "source": [
    "# Using an imported corpus to check for valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84363b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/devdutt/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7021e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "wordlist = set(brown.words())\n",
    "word_list = list(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754a99d",
   "metadata": {},
   "source": [
    "before we use this new word_list, however, we should do some cleanup to remove what it thinks are words but are actually punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83456345",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word.replace('*','') for word in word_list]\n",
    "word_list = [word.replace('[','') for word in word_list]\n",
    "word_list = [word.replace(']','') for word in word_list]\n",
    "word_list = [word.replace('?','') for word in word_list]\n",
    "word_list = [word.replace('.','') for word in word_list]\n",
    "word_list = [word.replace('+','') for word in word_list]\n",
    "word_list = [word.replace('/','') for word in word_list]\n",
    "word_list = [word.replace(';','') for word in word_list]\n",
    "word_list = [word.replace(':','') for word in word_list]\n",
    "word_list = [word.replace(',','') for word in word_list]\n",
    "word_list = [word.replace(')','') for word in word_list]\n",
    "word_list = [word.replace('(','') for word in word_list]\n",
    "word_list.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19669d9c",
   "metadata": {},
   "source": [
    "first we replaced all the symbols with a simple '' and then removed all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14230077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 16),\n",
       " (24, 30),\n",
       " (35, 45),\n",
       " (55, 68),\n",
       " (72, 78),\n",
       " (93, 105),\n",
       " (112, 119),\n",
       " (120, 123)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now rerunning the code to get a better output\n",
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]\n",
    "between_spaces_notvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18960220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check for the potential words that might be there in the substring\n",
    "\n",
    "partial_words = [loc for loc in locs if loc[0] in spacestarts_affine and loc[1] not in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06440606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(35, 38), (4, 7)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04744b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[35:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b28d82a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[4:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a462031",
   "metadata": {},
   "source": [
    "our locs variable contains the location of every word in the text\n",
    "\n",
    "it checks whether locs[0], the beginning of the word, is in space_starts_affine, a list containing the characters that come just after a space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3721e",
   "metadata": {},
   "source": [
    "then it checks whether the spacestarts does not have loc[1], which checks whether the word ends where a space begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49e550",
   "metadata": {},
   "source": [
    "If a word starts after a space and does not end at the same place as a space, we put it in our partial_words variable, because this could be a word that needs to have a space inserted after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baecc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will write a list comprehension that finds the ending location of every valid word that begin at the same location\n",
    "\n",
    "partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affine and loc[1] in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25329eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 16)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_words_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61ecbcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oneglimpse '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[35:46]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5606644",
   "metadata": {},
   "source": [
    "# FINDING FIRST AND SECOND HALVES OF POTENTIAL WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a42b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = between_spaces_notvalid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad8d0e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549e908",
   "metadata": {},
   "source": [
    "we will now check whether any of the words in partial_words could be the first half of oneperfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78a171",
   "metadata": {},
   "source": [
    "for a valid word to be the first half, it would have to have the same beginning location in the text, but not the same ending location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb8a4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "endsofbeginning= [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1] - loc[0])>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683136dc",
   "metadata": {},
   "source": [
    "we have specfied two things in the list which is that the word should have the same starting as that of the word in loc[0] and the other condition that is mentioned ensures that the word is not just one character long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f87332",
   "metadata": {},
   "source": [
    "The second condition is not necessary but it can help us avoid in geting false positives like avoid, aside, along, irate, iconic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c267b6",
   "metadata": {},
   "source": [
    "in the above examples the first letter of the word can itself be considered as a word itself but probably is not true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85470",
   "metadata": {},
   "source": [
    "our list endsofbeginnings should include the ending location of every valid word that begins at the same place as oneperfectly.\n",
    "\n",
    "now we will find the beginning location of every valid word. that ends at the same place as oneperfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3198c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1] - loc[0])>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8341eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beginningsofends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbbb5b",
   "metadata": {},
   "source": [
    "now we just need to find whether any locations are contained in both, endsofbeginnings and beginningsofends. If there are, that means that our invalid word is indeed a combination of 2 valid words without a space. We can use the intersection() function to find all the elements that are shared by both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7c46392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = list(set(endsofbeginning).intersection(beginningsofends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff550a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057d71b",
   "metadata": {},
   "source": [
    "It is possible that there is a case where the word might be from a brochure like choose spain! and instead gets interpreted as chooses pain.\n",
    "\n",
    "Hence a more sophesticated approach is to take into account the context - whether the words around choosespain tend to be about olives and bullfighting or about whips and superfluous dentist appointments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcfd72",
   "metadata": {},
   "source": [
    "However such an approach is difficult to do well and impossible to do perfectly, illustrating again the difficulty of language algorithms in general,\n",
    "\n",
    "In our case we will take the smallest element of pivot, not because this is certianly the correct one, but just because we have to take one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b384128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pivot = np.min(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd6e11",
   "metadata": {},
   "source": [
    "Finally replacing the invalids with the valids with a space in between in one liner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6400ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "textnew = text\n",
    "textnew = textnew.replace(text[loc[0]:loc[1]], text[loc[0]:pivot]+' '+text[pivot:loc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12559930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The one perfectly divine thing, the oneglimpse of God's paradisegiven on earth, is to fight a losingbattle - and notlose it.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57e99b",
   "metadata": {},
   "source": [
    "as we can see the code did work perfectly for oneperfectly and left all the others as it is since that was what the code was intended to do\n",
    "\n",
    "assembling all the algos together inside a function to cure the text completely we will get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37fed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertspaces(text,word_list):\n",
    "    locs = list(set([(m.start(),m.end()) for word in word_list for m in re.finditer(word, text)]))    \n",
    "    spacestarts = [m.start() for m in re.finditer(' ', text)]\n",
    "    spacestarts.append(-1)\n",
    "    spacestarts.append(len(text))\n",
    "    spacestarts.sort()\n",
    "    spacestarts_affine = [ss + 1 for ss in spacestarts]\n",
    "    spacestarts_affine.sort()\n",
    "    partial_words = [loc for loc in locs if loc[0] in spacestarts_affine and loc[1] not in spacestarts]\n",
    "    partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affine and loc[1] in spacestarts]\n",
    "    between_spaces = [(spacestarts[k] + 1, spacestarts[k+1]) for k in range(0, len(spacestarts) - 1)]\n",
    "    between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]\n",
    "    textnew = text\n",
    "    for loc in between_spaces_notvalid:\n",
    "        endsofbeginnings = [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1] - loc[0])>1]\n",
    "        beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1] - loc[0])>1]\n",
    "        pivot = list(set(endsofbeginnings).intersection(beginningsofends))\n",
    "        if len(pivot)>0:\n",
    "            pivot = np.min(pivot)\n",
    "            textnew = textnew.replace(text[loc[0]:loc[1]], text[loc[0]:pivot]+' '+text[pivot:loc[1]])\n",
    "    textnew = textnew.replace(' ',' ')\n",
    "    return textnew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a001e5",
   "metadata": {},
   "source": [
    "now we can define any text and call the function to add space automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa129882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one perfectly divine thing, the one glimpse of God's paradise given on earth, is to fight a losing battle - and not lose it.\n"
     ]
    }
   ],
   "source": [
    "print(insertspaces(text, word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f8427",
   "metadata": {},
   "source": [
    "#### we have successfully created an algorithm that can smartly insert spaces into english text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f5118",
   "metadata": {},
   "source": [
    "# Phrase completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d787c7",
   "metadata": {},
   "source": [
    "Apparently bulding this feature is simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddde5c5",
   "metadata": {},
   "source": [
    "we will start with a corpus but this time we are not just interested in the words but also how they fit together, so we will compile lists of n-grams from our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a80b7",
   "metadata": {},
   "source": [
    "An n-gram is simply a collection of n words that appear together. For eg, the phrase 'Reality is not always probable, or likely' is made up of seven words once spoken by someone great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b0ae6",
   "metadata": {},
   "source": [
    "A 1-gram is an individual word, so the 1-grams of the above phrase are all the words the above phrase consists of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad23363",
   "metadata": {},
   "source": [
    "The 2 grams have every string of 2 words for eg above is reality is, is not, not always, always probable... and so goes on for 3 grams and n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6feca8",
   "metadata": {},
   "source": [
    "## Tokenizing and getting N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd3050ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'forks', 'perpetually', 'toward', 'innumerable', 'futures']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = 'Time forks perpetually toward innumerable futures'\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26f70251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can get n-grams just this easily like this\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token, 2)\n",
    "trigrams = ngrams(token, 3)\n",
    "fourgrams = ngrams(token, 4)\n",
    "fivegrams = ngrams(token, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09743ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'forks', 'perpetually', 'toward', 'innumerable', 'futures']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "660e725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively we can put all the n-grams inside a list called grams\n",
    "\n",
    "grams = [ngrams(token,2),ngrams(token,3),ngrams(token,4),ngrams(token,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e044e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zip at 0x132cb07c0>,\n",
       " <zip at 0x132cb2640>,\n",
       " <zip at 0x132cb0c40>,\n",
       " <zip at 0x132cb2380>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f17c25",
   "metadata": {},
   "source": [
    "one corpus we could use is a collection of literary texts made available onine by google's peter norvig at some link\n",
    "\n",
    "for this chaper we will download Shakespear's complete works, available for free online at some link as well\n",
    "\n",
    "We read a corpus in python as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74fece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "file = requests.get('http://www.bradfordtuckfield.com/shakespeare.txt')\n",
    "file = file.text\n",
    "text = file.replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd876d",
   "metadata": {},
   "source": [
    "here we use requests to read a text file containing the collected works of shakespear from a websit where its being hosted, and then read into our python session in a variable called text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4089a8",
   "metadata": {},
   "source": [
    "now we rerun the code that created the grams variable. Here its with the new definition of the text variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd0f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token, 2)\n",
    "trigrams = ngrams(token, 3)\n",
    "fourgrams = ngrams(token, 4)\n",
    "fivegrams = ngrams(token, 5)\n",
    "grams = [ngrams(token, 1),ngrams(token, 2),ngrams(token, 3),ngrams(token, 4), ngrams(token, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b7616",
   "metadata": {},
   "source": [
    "now when the user searches for something, we will give him a suggestion of n+1-gram, essentially adding to his n- gram with our which most match all his entered words, that way we will be able to accomplish suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bee926",
   "metadata": {},
   "source": [
    "# Finding candidate n+1 grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f60c47",
   "metadata": {},
   "source": [
    "we can use the following simple lines to get the length of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e535e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "search_term = 'life is a'\n",
    "split_term = tuple(search_term.split(' '))\n",
    "search_term_length = len(search_term.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c393f",
   "metadata": {},
   "source": [
    "Now we need to use the most frequent appearing n+1-grams for suggesting so we will use a function called counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9555d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counted_grams = Counter(grams[search_term_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98db6f",
   "metadata": {},
   "source": [
    "This line has selected only the n+1 grams from our grams variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22753b0",
   "metadata": {},
   "source": [
    "Applying the Counter() function creates a list of tuples. Each tuple has an n+1 gram as its first element and the frequency of that n+1 gram in our corpus as its second elemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06d458cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('From', 'fairest', 'creatures', 'we'), 1)\n"
     ]
    }
   ],
   "source": [
    "print(list(counted_grams.items())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f00c78",
   "metadata": {},
   "source": [
    "This n-gram is the beginning of Shakespear's sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3ccce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('this', 'learning', 'mayst', 'thou')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(counted_grams)[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e77aa",
   "metadata": {},
   "source": [
    "we dont just want to search for the n-grams which are n+1 then the input but also find the most relatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f456521",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_terms = [element for element in list(counted_grams.items()) if element[0][:-1] == tuple(split_term)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0eaa57",
   "metadata": {},
   "source": [
    "This list comprehension iterates over every n+1 gram and calls each element as it does so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c130e1b",
   "metadata": {},
   "source": [
    "for each element it checks whether element[0][:-1]==tuple(split_term). The left side of this equality, element[0][:-1], simply takes the first n elements of each n+1 gram: the [:-1] is a handy way to disregard the last element of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528be3e5",
   "metadata": {},
   "source": [
    "The right side of the equality, tuple(split_term) is the n-gram which we are searching for ('life is a'). So we are checing for n+1 grams whose first n elements are the same as our n-gram of interest\n",
    "\n",
    "whichever terms match are stored in our final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6386b55",
   "metadata": {},
   "source": [
    "## Selecting a phrase based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa1a055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(matching_terms)>0:\n",
    "    frequencies = [item[1] for item in matching_terms]\n",
    "    maximum_frequency = np.max(frequencies)\n",
    "    highest_frequency_term = [item[0] for item in matching_terms if item[1] == maximum_frequency][0]\n",
    "    combined_terms = ' '.join(highest_frequency_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324799b",
   "metadata": {},
   "source": [
    "in the above code we defined frequencies, a list containing frequency of every n+1 gram in our corpus that matches the search term, then we used the numpy modul's max to get the highest of those frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec033b",
   "metadata": {},
   "source": [
    "we used another list comprehension to get the first n+1 gram that occurs with the highest frequency in the corpus, and finally we created a combined_term, a string that puts together all of the words in that search term, with spaces seperating the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5104c0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'life is a tedious'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716ee9c",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35a28e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_suggestion(search_term, text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    bigrams = ngrams(token, 2)\n",
    "    trigrams = ngrams(token, 3)\n",
    "    fourgrams = ngrams(token, 4)\n",
    "    fivegrams = ngrams(token, 5)\n",
    "    grams = [ngrams(token, 2), ngrams(token, 3),ngrams(token, 4),ngrams(token, 5)]\n",
    "    split_term = tuple(search_term.split(' '))\n",
    "    search_term_length = len(search_term.split(' '))\n",
    "    counted_grams = Counter(grams[search_term_length-1])\n",
    "    combined_term = 'No suggested searches'\n",
    "    matching_terms = [element for element in list(counted_grams.items()) if element[0][:-1] == tuple(split_term)]\n",
    "    if len(matching_terms)>0:\n",
    "        frequencies = [item[1] for item in matching_terms]\n",
    "        maximum_frequency = np.max(frequencies)\n",
    "        highest_frequency_term = [item[0] for item in matching_terms if item[1] == maximum_frequency][0]        \n",
    "        combined_term = ' '.join(highest_frequency_term)\n",
    "    return combined_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84d393c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life of a man\n"
     ]
    }
   ],
   "source": [
    "print(search_suggestion('life of a', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c601be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
